{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Код для поєднання датасетів у один великий (поєднує зображення і анотації)"
      ],
      "metadata": {
        "id": "x47rvYxIO3yA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "e63vAlPA3jTt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spgv916si3_W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "IQTlXqPZ4PQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_from_gdrive = True\n",
        "download_dataset_name = 'main_data.zip'\n",
        "file_url = 'https://drive.google.com/file/d/163a9Ee7VObzEfHA3DaLadEnvENROLooF/view?usp=sharing'\n",
        "\n",
        "files_to_add_to_dataset = ['/content/annot_3.zip']\n",
        "\n",
        "created_end_dataset_name = 'connected_dataset'\n",
        "download_after_creating = False\n",
        "set_to_drive = True\n",
        "drive_path = ''"
      ],
      "metadata": {
        "id": "TWMQ5-j44RjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some preprocessing"
      ],
      "metadata": {
        "id": "9o1NUZo65nv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if download_from_gdrive:\n",
        "  files_to_add_to_dataset += [os.path.join('/content', download_dataset_name)]\n",
        "\n",
        "created_end_dataset_name_with_zip = created_end_dataset_name + '.zip'\n",
        "created_end_dataset_path = os.path.join('/content', created_end_dataset_name_with_zip)"
      ],
      "metadata": {
        "id": "Hl00gm9s5s4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "5zBHlcFvzu9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import re"
      ],
      "metadata": {
        "id": "rUyE4HiR3oNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SqWimSky3VU"
      },
      "outputs": [],
      "source": [
        "def convert_drive_link(original_link):\n",
        "  if \"https://drive.google.com/uc?id=\" in original_link:\n",
        "    return original_link\n",
        "  original_link = original_link.replace('?usp=sharing', '').replace('?usp=drive_link', '')\n",
        "  pattern = r\"https://drive\\.google\\.com/file/d/([a-zA-Z0-9_-]+)/view\"\n",
        "\n",
        "  matcher = re.match(pattern, original_link)\n",
        "\n",
        "  if matcher:\n",
        "    file_id = matcher.group(1)\n",
        "    converted_link = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return converted_link\n",
        "  else:\n",
        "    raise Exception(f\"Not realized Google Drive link format.\\nGiven link is {original_link}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def install_from_google_drive(link, name, path=None, force_download = False):\n",
        "  full_path = name\n",
        "  if path is not None:\n",
        "    full_path = os.path.join(path, full_path)\n",
        "  if not force_download:\n",
        "    if os.path.exists(full_path):\n",
        "      print('The data already exists')\n",
        "      return\n",
        "\n",
        "  print('Start downloading')\n",
        "  gdown.download(convert_drive_link(link), full_path, quiet=False)\n",
        "  print('\\nDownloading have ended')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if download_from_gdrive:\n",
        "  install_from_google_drive(file_url, download_dataset_name)"
      ],
      "metadata": {
        "id": "iYUc6HBczwnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73ee35b-472e-4432-ee60-488f2df0de3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=163a9Ee7VObzEfHA3DaLadEnvENROLooF\n",
            "From (redirected): https://drive.google.com/uc?id=163a9Ee7VObzEfHA3DaLadEnvENROLooF&confirm=t&uuid=70a6606d-7f26-4bbb-8cfc-aead9dfe9511\n",
            "To: /content/main_data.zip\n",
            "100%|██████████| 95.7M/95.7M [00:01<00:00, 55.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading have ended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some functions"
      ],
      "metadata": {
        "id": "ECafzSQn6gSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indent(elem, level=0):\n",
        "  i = \"\\n\" + level*\"  \"\n",
        "  if len(elem):\n",
        "    if not elem.text or not elem.text.strip():\n",
        "      elem.text = i + \"  \"\n",
        "    if not elem.tail or not elem.tail.strip():\n",
        "      elem.tail = i\n",
        "    for elem in elem:\n",
        "      indent(elem, level+1)\n",
        "    if not elem.tail or not elem.tail.strip():\n",
        "      elem.tail = i\n",
        "  else:\n",
        "    if level and (not elem.tail or not elem.tail.strip()):\n",
        "      elem.tail = i\n",
        "\n",
        "def get_max_id(root):\n",
        "    \"\"\"Get the maximum value of the id attribute among image elements.\"\"\"\n",
        "    max_id = 0\n",
        "    for image in root.findall(\"image\"):\n",
        "        image_id = int(image.get(\"id\"))\n",
        "        if image_id > max_id:\n",
        "            max_id = image_id\n",
        "    return max_id\n",
        "\n",
        "def add_anotations_to_element_tree(new_root, path_to_annotations, image_dir_name='image'):\n",
        "  # Parse the input XML file\n",
        "  ident = get_max_id(new_root)\n",
        "  tree = ET.parse(path_to_annotations)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  # Copy image elements from the original file to the new file\n",
        "  for image in root.findall(\"image\"):\n",
        "      new_image = ET.SubElement(new_root, \"image\")\n",
        "      for attr in image.attrib:\n",
        "          if attr == 'id':\n",
        "            ident += 1\n",
        "            new_image.set(attr, str(ident))\n",
        "          elif attr == 'name':\n",
        "            old_name = image.get(attr).split('/')\n",
        "            if len(old_name) > 1:\n",
        "              old_name = old_name[1:]\n",
        "            new_name = [image_dir_name] + old_name\n",
        "            new_name = '/'.join(new_name)\n",
        "            new_image.set(attr, new_name)\n",
        "          else:\n",
        "            new_image.set(attr, image.get(attr))\n",
        "\n",
        "      # Copy polygon elements from the original image to the new image\n",
        "      for polygon in image.findall(\"polygon\"):\n",
        "          new_polygon = ET.SubElement(new_image, \"polygon\")\n",
        "          for attr in polygon.attrib:\n",
        "              new_polygon.set(attr, polygon.get(attr))"
      ],
      "metadata": {
        "id": "f48BtxZe1ot0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_files(source_dir, dest_dir, verbose=True):\n",
        "    # Iterate through files in the source directory\n",
        "    copy_num, exist_num = 0, 0\n",
        "    for filename in os.listdir(source_dir):\n",
        "        source_file = os.path.join(source_dir, filename)\n",
        "        dest_file = os.path.join(dest_dir, filename)\n",
        "\n",
        "        # Check if the file already exists in the destination directory\n",
        "        if not os.path.exists(dest_file):\n",
        "            # Copy the file from source to destination\n",
        "            shutil.copy2(source_file, dest_dir)\n",
        "            copy_num += 1\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(f\"File '{filename}' already exists in '{dest_dir}', skipping.\")\n",
        "                exist_num += 1\n",
        "    if verbose:\n",
        "        print(f\"Copied num: {copy_num}, already exists: {exist_num}\")\n"
      ],
      "metadata": {
        "id": "u8ZWihOe04x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zip_folder(folder_path, zip_path):\n",
        "    # Create a ZipFile object in write mode\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "        # Walk through all the files and subdirectories in the given folder\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            # Iterate through each file\n",
        "            for file in files:\n",
        "                # Get the full path of the file\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Add the file to the zip archive\n",
        "                zipf.write(file_path, os.path.relpath(file_path, folder_path))\n",
        "\n",
        "def extract_zip(zip_file, extract_to):\n",
        "    # Create a ZipFile object in read mode\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        # Extract all contents of the zip file to the specified directory\n",
        "        zip_ref.extractall(extract_to)"
      ],
      "metadata": {
        "id": "7etXKTfK8mrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_shit_if_exist(path_to_dir, shit_files=['.DS_Store'], shit_dirs=['__MACOSX']):\n",
        "  dir_files = os.listdir(path_to_dir)\n",
        "  for sh in shit_files:\n",
        "    if sh in dir_files:\n",
        "      sh_path = os.path.join(path_to_dir, sh)\n",
        "      os.remove(sh_path)\n",
        "\n",
        "  for sh in shit_dirs:\n",
        "    if sh in dir_files:\n",
        "      sh_path = os.path.join(path_to_dir, sh)\n",
        "      shutil.rmtree(sh_path)\n"
      ],
      "metadata": {
        "id": "ASmKqD9QKwNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_big_dataset_from_parts(path_to_folders, dataset_name='connected_dataset', for_extracted='exctracted', to_zip=False, force=False, verbose=True):\n",
        "  images_dataset_path = os.path.join(dataset_name, 'image')\n",
        "  annotations_dataset_path = os.path.join(dataset_name, 'annotations.xml')\n",
        "  new_root = ET.Element(\"annotations\")\n",
        "\n",
        "  if not os.path.exists(dataset_name):\n",
        "    os.mkdir(dataset_name)\n",
        "    os.mkdir(images_dataset_path)\n",
        "  elif force:\n",
        "    shutil.rmtree(dataset_name)\n",
        "\n",
        "    os.mkdir(dataset_name)\n",
        "    os.mkdir(images_dataset_path)\n",
        "  else:\n",
        "    daset_dir_elements = os.listdir(dataset_name)\n",
        "    if len(daset_dir_elements) == 0:\n",
        "      os.mkdir(images_dataset_path)\n",
        "    elif len(daset_dir_elements) == 1:\n",
        "      raise Exception(f'Folder {dataset_name} has 1 element, not realized yet')\n",
        "    elif len(daset_dir_elements) == 2:\n",
        "      # TODO: realize it !!!\n",
        "      raise Exception(f'Folder {dataset_name} has 2 elements, not realized yet')\n",
        "    else:\n",
        "      raise Exception(f'Folder {dataset_name} has more than 2 elements, not realized yet')\n",
        "\n",
        "  for path_to_folder in path_to_folders:\n",
        "    if path_to_folder.endswith('.zip'):\n",
        "      folder_name = path_to_folder.split('/')[-1][:-4]\n",
        "      path_to_zip = os.path.join(for_extracted, folder_name)\n",
        "      extract_zip(path_to_folder, path_to_zip)\n",
        "      remove_shit_if_exist(path_to_zip)\n",
        "      if len(os.listdir(path_to_zip)) == 1:\n",
        "        some_crutch_path = os.path.join(path_to_zip, os.listdir(path_to_zip)[0])\n",
        "        remove_shit_if_exist(some_crutch_path)\n",
        "        inner_folder = os.listdir(some_crutch_path)\n",
        "        if len(inner_folder) == 2:\n",
        "          path_to_zip = some_crutch_path\n",
        "\n",
        "      path_to_folder = path_to_zip\n",
        "    remove_shit_if_exist(path_to_folder)\n",
        "    folder_files = os.listdir(path_to_folder)\n",
        "    if len(folder_files) > 2:\n",
        "      raise Exception(f'Folder {path_to_folder} has more than 2 elements: {folder_files}\\nIt must has only folder with images + xml with annotations')\n",
        "    elif len(folder_files) < 2:\n",
        "      raise Exception(f'Folder {path_to_folder} has less than 2 elements: {folder_files}\\nIt must has folder with images + xml with annotations')\n",
        "\n",
        "    annot_file = [i for i in folder_files if i.endswith('.xml')]\n",
        "    if len(annot_file) > 1:\n",
        "      raise Exception(f'Folder {path_to_folder} has more than 1 xml: {annot_file}')\n",
        "    elif len(annot_file) < 1:\n",
        "      raise Exception(f'Folder {path_to_folder} hasn`t xml with annotations')\n",
        "\n",
        "    annot_file = os.path.join(path_to_folder, annot_file[0])\n",
        "\n",
        "    folder_with_images = [i for i in folder_files if not i.endswith('.xml')][0]\n",
        "    folder_with_images = os.path.join(path_to_folder, folder_with_images)\n",
        "\n",
        "    copy_files(folder_with_images, images_dataset_path, verbose=verbose)\n",
        "    add_anotations_to_element_tree(new_root, annot_file)\n",
        "\n",
        "    if verbose:\n",
        "      print(f'Dataset {path_to_folder} added')\n",
        "\n",
        "  indent(new_root)\n",
        "  new_tree = ET.ElementTree(new_root)\n",
        "  new_tree.write(annotations_dataset_path, encoding=\"utf-8\", xml_declaration=True)\n",
        "\n",
        "  if to_zip:\n",
        "    zip_folder(dataset_name, dataset_name + '.zip')"
      ],
      "metadata": {
        "id": "x1orScE0ujbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conecting"
      ],
      "metadata": {
        "id": "sBzUbfEG6jfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_big_dataset_from_parts(files_to_add_to_dataset, created_end_dataset_name, to_zip=True, force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNILEUrXrjWF",
        "outputId": "b2f136b9-d30e-417b-fad2-d301bf29cdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied num: 56, already exists: 0\n",
            "Dataset exctracted/annot_3 added\n",
            "Copied num: 267, already exists: 0\n",
            "Dataset exctracted/main_data added\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "CwIX-lKZ7Cez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "if download_after_creating:\n",
        "  files.download(created_end_dataset_name_with_zip)"
      ],
      "metadata": {
        "id": "Mw8aYht4HIgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set to gd"
      ],
      "metadata": {
        "id": "leXVBaao7FRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "if set_to_drive:\n",
        "  drive.mount('/content/drive')\n",
        "  data_gd_path = os.path.join(drive_path, created_end_dataset_name_with_zip)\n",
        "  if not os.path.exists(drive_path):\n",
        "    raise Exception(f'Can`t find gd path {drive_path}')\n",
        "  if os.path.exists(data_gd_path):\n",
        "    os.remove(data_gd_path)\n",
        "\n",
        "  shutil.copy2(created_end_dataset_path, data_gd_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6csgNIm24Mfb",
        "outputId": "33f25656-3306-4b3b-f39c-19647b45885a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fIvHSpgY747y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}